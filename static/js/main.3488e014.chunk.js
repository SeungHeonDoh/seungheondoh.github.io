(this.webpackJsonpseungheondoh=this.webpackJsonpseungheondoh||[]).push([[0],{22:function(e){e.exports=JSON.parse('[{"title":"Enriching Music Descriptions with a Finetuned-LLM and Metadata for Text-to-Music Retrieval","category":"ann_ret nlp","year":"2024","Authors":"SeungHeon Doh, Minhee Lee, Dasaem Jeong, Juhan Nam","bookTitle":"Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2024 (Submitted)","bookCategory":"Conferences and Workshops","material":{"demo":"https://seungheondoh.github.io/music-text-representation-pp-demo/"}},{"title":"MuLaB: a Benchmark Suite and Toolkit for Evaluating Music-and-Language Models","category":"ann_ret nlp","year":"2023","Authors":"Ilaria Manco, SeungHeon Doh, Benno Weck, Minz Won, Yixiao Zhang, Dmitry Bogdanov, Yusong Wu, Ke Chen, Philip Tovstogan, Emmanouil Benetos, Elio Quinton, George Fazekas, Juhan Nam","bookTitle":"NeurIPS 2023 Track Datasets and Benchmarks, 2023 (Submitted)","bookCategory":"Conferences and Workshops","material":{"dataset":"https://huggingface.co/mulab"}},{"title":"Annotator Subjectivity in the MusicCaps Dataset","category":"ann_ret nlp","year":"2023","Authors":"Minhee Lee, Seungheon Doh and Dasaem Jeong","bookTitle":"2nd Workshop on Human-Centric Music Information Research (HCMIR), 2023","bookCategory":"Conferences and Workshops","material":{"update soon":"#"}},{"title":"LP-MusicCaps: LLM-based Pseudo Music Captioning","category":"ann_ret nlp","year":"2023","Authors":"Seungheon Doh, Keunwoo Choi, Jongpil Lee, Juhan Nam","bookTitle":"Proceedings of the 24nd International Society for Music Information Retrieval Conference (ISMIR), 2023","bookCategory":"Conferences and Workshops","material":{"pdf":"https://arxiv.org/abs/2307.16372","code":"https://github.com/seungheondoh/lp-music-caps","dataset":"https://huggingface.co/papers/2307.16372","demo":"https://huggingface.co/spaces/seungheondoh/LP-Music-Caps-demo"}},{"title":"Toward Universal Text-to-Music Retrieval","category":"ann_ret nlp","year":"2023","Authors":"SeungHeon Doh, Minz Won, Keunwoo Choi, Juhan Nam","bookTitle":"Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2023","bookCategory":"Conferences and Workshops","material":{"pdf":"https://arxiv.org/abs/2211.14558","code":"https://github.com/seungheondoh/music-text-representation","demo":"https://seungheondoh.github.io/text-music-representation-demo/","dataset":"https://github.com/SeungHeonDoh/msd-subsets/"}},{"title":"Textless Speech-to-Music Retrieval Using Emotion Similarity","category":"ann_ret audio","year":"2023","Authors":"SeungHeon Doh, Minz Won, Keunwoo Choi, Juhan Nam","bookTitle":"Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2023","bookCategory":"Conferences and Workshops","material":{"pdf":"https://arxiv.org/abs/2303.10539","demo":"https://seungheondoh.github.io/speech-to-music-demo/"}},{"title":"Music Playlist Title Generation Using Artist Information","category":"ann_ret nlp","year":"2023","Authors":"Haven Kim, Seungheon Doh, Junwon Lee, Juhan Nam","bookTitle":"AAAI-23 Workshop on Creative AI Across Modalities","bookCategory":"Conferences and Workshops","material":{"pdf":"https://arxiv.org/abs/2301.08145"}},{"title":"Hi, KIA: A Speech Emotion Recognition Dataset for Wake-Up Words","category":"ann_ret audio","year":"2022","Authors":"Taesu Kim*, Seungheon Doh*, Gyunpyo Lee, Hyung seok Jun, Juhan Nam, Hyeon-Jeong Suk (*equally contribution)","bookTitle":"Proceedings of the 14th Asia Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA), 2022","bookCategory":"Conferences and Workshops","material":{"pdf":"https://arxiv.org/abs/2211.03371","dataset":"https://zenodo.org/record/6989810"}},{"title":"Million Song Search: Web Interface for Semantic Music Search Using Musical Word Embedding","category":"ann_ret nlp","year":"2021","Authors":"SeungHeon Doh, Jongpil Lee, and Juhan Nam","bookTitle":"Late Breaking Demo in the 22st International Society for Music Information Retrieval Conference (ISMIR), 2021","bookCategory":"Conferences and Workshops","material":{"pdf":"https://archives.ismir.net/ismir2021/latebreaking/000053.pdf","demo":"http://millionsongsearch.kaist.ac.kr/"}},{"title":"Music Playlist Title Generation: A Machine-Translation Approach","category":"ann_ret nlp","year":"2021","Authors":"SeungHeon Doh, Junwon Lee, and Juhan Nam","bookTitle":"2nd Workshop on Natural Language Processing for Music and Spoken Audio (NLP4MusA), 2021","bookCategory":"Conferences and Workshops","material":{"pdf":"https://arxiv.org/abs/2110.07354"}},{"title":"EMOPIA: A Multi-Modal Pop Piano Dataset For Emotion Recognition and Emotion-based Music Generation","category":"ann_ret gen","year":"2021","Authors":"Hung, Hsiao-Tzu and Ching, Joann and Doh, SeungHeon and Kim, Nabin and Nam, Juhan and Yang, Yi-Hsuan","bookTitle":"Proceedings of the 22nd International Society for Music Information Retrieval Conference (ISMIR), 2021","bookCategory":"Conferences and Workshops","material":{"pdf":"https://arxiv.org/abs/2108.01374","demo":"https://annahung31.github.io/EMOPIA//"}},{"title":"Tr\xe4umerAI: Dreaming Music with StyleGAN","category":"gen cv","year":"2021","Authors":"Dasaem Jeong, SeungHeon Doh, and Taegyun Kwon","bookTitle":"Workshop on Machine Learning for Creativity and Design, Neural Information Processing Systems (NeurIPS), 2020","bookCategory":"Conferences and Workshops","material":{"pdf":"https://arxiv.org/abs/2102.04680","demo":"https://jdasam.github.io/traeumerAI_demo/"}},{"title":"Musical Word Embedding: Bridging the Gap between Listening Contexts and Music","category":"ann_ret nlp","year":"2020","Authors":"SeungHeon Doh, Jongpil Lee, Tae Hong Park, and Juhan Nam","bookTitle":"Machine Learning for Media Discovery Workshop, International Conference on Machine Learning (ICML), 2020","bookCategory":"Conferences and Workshops","material":{"pdf":"https://arxiv.org/abs/2008.01190","demo":"https://seungheondoh.github.io/MusicWordVec"}}]')},23:function(e){e.exports=JSON.parse('[{"_id":"enrich2024doh","title":"TTMR++: Enriching Music Descriptions with a Finetuned-LLM and Metadata","decs":"this paper proposes an improved Text-to-Music Retrieval model, denoted as TTMR++, which utilizes rich text descriptions generated with a finetuned large language model and metadata.","category":"research","url":"https://seungheondoh.github.io/text-music-representation-demo-pp/","year":"2024"},{"_id":"lp2023doh","title":"LP-MusicCaps: LLM-Based Pseudo Music Captioning","decs":"We propose the use of large language models (LLMs) to artificially generate the description sentences from large-scale tag datasets. This results in approximately 2.2M captions paired with 0.5M audio clips. We trained a transformer-based music captioning model.","category":"research","url":"https://github.com/seungheondoh/lp-music-caps/","year":"2023"},{"_id":"toward2023doh","title":"Toward Universal Text-to-Music Retrieval","decs":"This paper introduces effective design choices for text-to-music retrieval systems. An ideal text-based retrieval system would support various input queries such as pre-defined tags, unseen tags, and sentence-level descriptions.","category":"research","url":"https://seungheondoh.github.io/text-music-representation-demo/","year":"2023"},{"_id":"speech2023doh","title":"Speech to Music through Emotion","decs":"Automatic speech emotion recognition (SER) can be used as a music recommendation application for content creators. In this paper, our goal is to help creators find music to match the emotion of their speech.","category":"research","url":"https://seungheondoh.github.io/speech-to-music-demo/","year":"2023"},{"_id":"musical_word_embedding","title":"Musical Word Embedding for Music Annotation and Retrieval","decs":"Musical Word Embedding use a wide spectrum of text corpus from general to music-specific words and defining musical specificity of the corpus as a measure of how the semantics of words is specific to the songs or far from it.","category":"research","url":"https://seungheondoh.github.io/musical_word_embedding_demo/","year":"2022"}]')},32:function(e){e.exports=JSON.parse('[{"id":1,"title":"Seungheon Doh","describtion":"I\'m a Ph.D Student at Music and Audio Computing Lab, working on Music, ML/DL.","detail":"I\'m working on Music and Machine Learning, with a specific focus on advancing our understanding of machines\' ability to perceive music, articulate musical experiences using natural language, and generate visual representations. My primary research efforts are concentrated on advancing the domain of representation learning for music and multimodal media","img":"Seungheon_Doh.jpg","ko_name":"\ub3c4\uc2b9\ud5cc","en_name":"Seungheon Doh","material":{"google scholar":"https://scholar.google.com/citations?user=MCkggcgAAAAJ&hl","github":"https://github.com/seungheondoh","linkedin":"https://www.linkedin.com/in/dohppak/","twitter":"https://twitter.com/SeungHeon_Doh"}}]')},33:function(e){e.exports=JSON.parse('[{"date":"Sep-2023","contents":"Oen paper has been submitted to ICASSP 2024 (Proceedings)","link":""},{"date":"May-2023","contents":"LP-MusicCaps: LLM-based Pseudo Music Captioning has been accepted to ISMIR 2023 (Proceedings)","link":""},{"date":"Mar-2023","contents":"Musical Word Embedding for Music Tagging and Retrieval has been submitted to IEEE TASLP","link":""},{"date":"Dec-2022","contents":"I\'m starting a research internship at NaverCorp (Now AI Team). Collaborate with Jeong Choi","link":"https://scholar.google.co.kr/citations?user=0r1yuiMAAAAJ&hl=ko"},{"date":"May-2022","contents":"As a visiting ph.D student, I will visit the NYU Data Science Center. Collaborate with Prof. Kyunghyun Cho.","link":"https://kyunghyuncho.me/"},{"date":"Jul-2021","contents":"I\'m starting a research internship at ByteDance (Speech, Audio, and Music Intelligence Team). Collaborate with Keunwoo Choi, and MinzWon","link":"https://keunwoochoi.github.io/"}]')},34:function(e){e.exports=JSON.parse('[{"id":"1","title":"All","filter":"*"},{"id":"2","title":"Annotation and Retrieval","filter":"ann_ret"},{"id":"3","title":"Generation","filter":"gen"},{"id":"4","title":"Language & Music","filter":"nlp"},{"id":"5","title":"Vision & Music","filter":"cv"},{"id":"6","title":"Audio & Music","filter":"audio"}]')},35:function(e){e.exports=JSON.parse('{"Teaching":[{"date":"Mar-2023","contents":"TA, GCT731 Topics in Music Technology: Generative AI for Music, KAIST GSCT","link":""},{"date":"Sep-2022","contents":"TA, GCT634 Musical Applications of Machine Learning, KAIST GSCT","link":""},{"date":"Sep-2021","contents":"TA, GCT634 Musical Applications of Machine Learning, KAIST GSCT","link":""},{"date":"Sep-2020","contents":"TA, GCT731 Topics in Music Technology: Cognitive Science of Music, KAIST GSCT","link":""},{"date":"Sep-2019","contents":"TA, GCT576 Social Computing, KAIST GSCT","link":""}],"Talk":[{"date":"Mar-2023","contents":"Multimodal Music Retrieval for Listener and Contents Creator, Seoul Univ. MARG","link":""},{"date":"Dec-2022","contents":"Music Informational Retrieval with Natural Language Processing, YONSEI Univ.","link":""},{"date":"May-2020","contents":"Digital Signal Processing and Speech Recognition, SK planet","link":"https://www.youtube.com/watch?v=RxbkEjV7c0o&t=111s"},{"date":"Aug-2019","contents":"Pycon Tutorial: Music and Deep Learning, PyconKR","link":"https://github.com/Dohppak/Pycon_Tutorial_Music_DeepLearing"}],"Service":[{"date":"Sep-2020","contents":"Korean translator, NYU Deep Learning DS-GA 1008, Yann LeCun & Alfredo Canziani","link":"https://github.com/Atcold/pytorch-Deep-Learning"}]}')},36:function(e){e.exports=JSON.parse('[{"id":"1","school":"Korea Advanced Institute of Science and Technology (KAIST)","position":"Ph.D Student in Graduate School of Culture Technology","duration":"2021 - present","advisor":"@ Music and Audio Computing Lab (Advisor: Juhan Nam)"},{"id":"2","school":"Korea Advanced Institute of Science and Technology (KAIST)","position":"MSc. in Graduate School of Culture Technology","duration":"2019 - 2021","advisor":"@ Music and Audio Computing Lab (Advisor: Juhan Nam)","thesis":"Musical Word Embedding for Natural Language based Music Annotation and Retrieval"},{"id":"3","school":"Ulsan National Institute of Science and Technology (UNIST)","position":"B.S. in School of Business administration & Industrial Design","duration":"2014 - 2019"}]')},37:function(e){e.exports=JSON.parse('[{"id":"1","institution":"NaverCorp","location":"1784, South Korea","position":"Research Intern in Now AI Team","duration":"Dec 2022 - Feb 2023","advisor":" (Advisor: Jeong Choi)"},{"id":"2","institution":"Computational Intelligence, Vision, and Robotics Lab (CILVR)","location":"Center for Data Science, New York University, United States","position":"Visiting Student","duration":"Jun 2022 - Aug 2022","advisor":" (Advisor: Kyunghyun Cho)"},{"id":"3","institution":"ByteDance","location":"Remote (due to COVID-19)","position":"Research Intern in Speech, Audio & Music Intelligence Team","duration":"Jul 2021 - Jan 2022","advisor":" (Advisor: Keunwoo Choi, Minz Won)"},{"id":"4","institution":"Music and Audio Research Laboratory (MARL)","location":"Steinhardt, New York University, United States","position":"Visiting Student","duration":"Dec 2019 - Feb 2020","advisor":"(Advisor: TaeHong Park)"}]')},38:function(e){e.exports=JSON.parse('[{"id":"1","title":"All","filter":"*"},{"id":"2","title":"Review","filter":"review"},{"id":"3","title":"Research","filter":"research"}]')},62:function(e,t,n){},63:function(e,t,n){"use strict";n.r(t);var i=n(1),o=n.n(i),a=n(21),s=n.n(a),c=n(39),r=n(2),l=n(10),d=n.n(l),h=n(0),u=function(){var e=window.location.href.split("https://seungheondoh.github.io/");return Object(h.jsx)("header",{id:"header",className:"site-header",children:Object(h.jsxs)("div",{className:"wrapper d-flex justify-content-between",children:[Object(h.jsx)("div",{className:"align-self-center",children:Object(h.jsx)("p",{children:"  "})}),Object(h.jsx)("nav",{className:"menu-third",children:Object(h.jsxs)("ul",{className:"clearfix list-unstyled",children:[Object(h.jsx)("li",{className:"menu-item"+("#/"===e[1]?" current-menu-item":""),children:Object(h.jsx)("a",{title:"Home",className:"btn btn-link transform-scale-h border-0 p-0",href:"#/",children:"Home"})}),Object(h.jsx)("li",{className:"menu-item"+("#/blog"===e[1]?" current-menu-item":""),children:Object(h.jsx)("a",{title:"blog",className:"btn btn-link transform-scale-h border-0 p-0",href:"#/blog",children:"Blog"})})]})})]})})},m=function(){return Object(h.jsx)("footer",{id:"footer",className:"site-footer",children:Object(h.jsx)("div",{className:"wrapper no-space",children:Object(h.jsx)("div",{className:"row"})})})},g=function(e){var t=e.keyword,n=e.link,i=e.position,o=e.textcolor,a=e.backgroundcolor,s="btn ".concat(i," has-text-color ").concat(o," has-background ").concat(a);return Object(h.jsx)("a",{href:n,className:s,children:Object(h.jsx)("b",{children:t})})},b=function(e){var t=e.keyword,n=e.link,i=e.position,o=e.textcolor,a=e.backgroundcolor,s="btn ".concat(i," has-text-color ").concat(o," has-background ").concat(a);return Object(h.jsx)("a",{href:n,download:"CV_seunghenodoh",className:s,children:Object(h.jsx)("b",{children:t})})},j=function(e){var t=e.ProfData;return Object(h.jsx)("section",{id:"page-content",className:"spacer",children:Object(h.jsx)("div",{className:"peoplecard",children:Object(h.jsx)("div",{className:"wrapper",children:Object(h.jsx)("div",{className:"prof_cardwrapper",children:t.map((function(e){return Object(h.jsxs)("div",{className:"img_div",children:[Object(h.jsx)("img",{className:"prof_img",src:"/assets/img/people/"+e.img,alt:e.title}),Object(h.jsxs)("div",{className:"info_div",children:[Object(h.jsx)("h4",{children:e.title}),Object(h.jsxs)("p",{className:"p",children:[" I'm a Ph.D Student at ",Object(h.jsx)("a",{href:"https://mac.kaist.ac.kr/",children:"Music and Audio Computing Lab"}),", advised by ",Object(h.jsx)("a",{href:"https://mac.kaist.ac.kr/~juhan/",children:"Prof. Juhan Nam"}),". ",Object(h.jsx)("br",{}),"I'm working on Music and Machine Learning. My research is centered on the capabilities of machines to perceive music, express musical experiences in natural language, and represent them visually. My research primarily revolves around the domain of representation learning in the context of music and multimodal media."]}),Object(h.jsxs)("div",{className:"btn_div",children:[Object(h.jsx)(b,{keyword:"cv",link:"/assets/cv/CV_seungheon.pdf",position:"",textcolor:"has-white-color",backgroundcolor:"has-olive-background-color"}),Object.keys(e.material).map((function(t,n){return Object(h.jsx)(g,{keyword:t,link:e.material[t],position:"inline",textcolor:"has-white-color",backgroundcolor:"has-gray-dark-background-color"})}))]})]})]})}))})})})})},p=n(32),x=function(e){var t=e.NewsInfoData;return Object(h.jsx)("section",{id:"page-content",className:"spacer p-top-lg p-bottom-lg",children:Object(h.jsx)("div",{id:"blog",children:Object(h.jsxs)("div",{className:"news wrapper",children:[Object(h.jsx)("h4",{children:"News"}),Object(h.jsx)("ul",{children:t.map((function(e){return""===e.link?Object(h.jsx)(h.Fragment,{children:Object(h.jsxs)("li",{children:[" ",e.date," | ",e.contents]})}):Object(h.jsx)(h.Fragment,{children:Object(h.jsxs)("li",{children:[" ",e.date," | ",e.contents,Object(h.jsx)(g,{keyword:"Link",link:e.link,position:"inline",textcolor:"has-white-color",backgroundcolor:"has-gray-dark-background-color"})]})})}))}),Object(h.jsx)("hr",{})]})})})},f=n(33),O=n(14),v=n(20),k=n(9),y=n(15),w=n(19),S=n(17),M=n.n(S),C=n(34),N=n(22),T=function(e){Object(y.a)(n,e);var t=Object(w.a)(n);function n(e){var i;return Object(O.a)(this,n),(i=t.call(this,e)).onFilterChange=function(e){var t=i.grid;void 0===i.iso&&(i.iso=new M.a(t,{itemSelector:".publicationTable-item",masonry:{horizontalOrder:!0}})),"*"===e?i.iso.arrange({filter:"*"}):i.iso.arrange({filter:".".concat(e)})},i.onFilterChange=i.onFilterChange.bind(Object(k.a)(i)),i.state={selected:0,list:C},i}return Object(v.a)(n,[{key:"handleClick",value:function(e,t){return t.preventDefault(),this.setState({selected:e}),!1}},{key:"componentDidMount",value:function(){}},{key:"render",value:function(){var e=this,t=this.state.list.length-1;return Object(h.jsx)("div",{className:"publicationTable spacer p-bottom-lg",children:Object(h.jsxs)("div",{className:"wrapper",children:[Object(h.jsx)("h4",{children:"Publications"}),Object(h.jsx)("ul",{className:"publicationTable-filter",children:this.state.list.map((function(n,i){return Object(h.jsxs)(o.a.Fragment,{children:[Object(h.jsx)("li",{children:Object(h.jsx)("span",{title:n.title,className:"btn btn-link transform-scale-h click"+(i===e.state.selected?" active":""),"data-filter":n.filter,onClick:function(t){e.onFilterChange(n.filter),e.handleClick(i,t)},children:n.title})}),i!==t?Object(h.jsx)("li",{children:Object(h.jsx)("span",{className:"btn btn-link",children:"-"})}):""]},i)}))}),Object(h.jsx)("div",{className:"publicationTable-item-wrapper",children:Object(h.jsx)("div",{className:"publicationTable-items",ref:function(t){return e.grid=t},children:N&&N.map((function(e,t){return""===e.material?Object(h.jsxs)("div",{title:e.title,className:"publicationTable-item active "+e.category,children:[Object(h.jsx)("h6",{children:e.title}),Object(h.jsx)("p",{className:"no-line-hight",children:e.Authors}),Object(h.jsx)("p",{className:"date",children:e.bookTitle})]},t):Object(h.jsxs)("div",{title:e.title,className:"publicationTable-item active "+e.category,children:[Object(h.jsx)("h6",{children:e.title}),Object(h.jsx)("p",{className:"no-line-hight",children:e.Authors}),Object(h.jsx)("p",{className:"date",children:e.bookTitle}),Object.keys(e.material).map((function(t,n){return 0===n?Object(h.jsx)(g,{keyword:t,link:e.material[t],position:"",textcolor:"has-white-color",backgroundcolor:"has-gray-dark-background-color"}):Object(h.jsx)(g,{keyword:t,link:e.material[t],position:"inline",textcolor:"has-white-color",backgroundcolor:"has-gray-dark-background-color"})}))]},t)}))})})]})})}}]),n}(i.Component),A=function(e){var t=e.ExpInfoData,n=t.Service,i=t.Talk,o=t.Teaching;return Object(h.jsx)("section",{id:"page-content",className:"spacer p-bottom-lg",children:Object(h.jsx)("div",{id:"blog",children:Object(h.jsxs)("div",{className:"wrapper",children:[Object(h.jsx)("h4",{children:"Service, Talk, Teaching"}),Object(h.jsxs)("div",{className:"experience",children:[Object(h.jsx)("h6",{children:"Service"}),n.map((function(e){return""===e.link?Object(h.jsx)(h.Fragment,{children:Object(h.jsxs)("p",{children:[" ",e.date," | ",e.contents]})}):Object(h.jsx)(h.Fragment,{children:Object(h.jsxs)("p",{children:[" ",e.date," | ",e.contents,Object(h.jsx)(g,{keyword:"Link",link:e.link,position:"inline",textcolor:"has-white-color",backgroundcolor:"has-gray-dark-background-color"})]})})})),Object(h.jsx)("h6",{children:"Talk"}),i.map((function(e){return""===e.link?Object(h.jsx)(h.Fragment,{children:Object(h.jsxs)("p",{children:[" ",e.date," | ",e.contents]})}):Object(h.jsx)(h.Fragment,{children:Object(h.jsxs)("p",{children:[" ",e.date," | ",e.contents,Object(h.jsx)(g,{keyword:"Link",link:e.link,position:"inline",textcolor:"has-white-color",backgroundcolor:"has-gray-dark-background-color"})]})})})),Object(h.jsx)("h6",{children:"Teaching"}),o.map((function(e){return""===e.link?Object(h.jsx)(h.Fragment,{children:Object(h.jsxs)("p",{children:[" ",e.date," | ",e.contents]})}):Object(h.jsx)(h.Fragment,{children:Object(h.jsxs)("p",{children:[" ",e.date," | ",e.contents,Object(h.jsx)(g,{keyword:"Link",link:e.link,position:"inline",textcolor:"has-white-color",backgroundcolor:"has-gray-dark-background-color"})]})})}))]})]})})})},I=n(35),L=function(e){var t=e.EduInfoData;return Object(h.jsx)("section",{id:"page-content",className:"spacer p-bottom-lg",children:Object(h.jsx)("div",{id:"blog",children:Object(h.jsxs)("div",{className:"eduacation wrapper",children:[Object(h.jsx)("h4",{children:"Education"}),Object(h.jsx)("div",{className:"eduacation",children:t.map((function(e){return Object(h.jsxs)(h.Fragment,{children:[Object(h.jsx)("h6",{children:e.school}),Object(h.jsxs)("p",{children:[e.position," ",""!==e.advisor?Object(h.jsx)("p",{children:e.advisor}):null]}),Object(h.jsx)("p",{className:"date",children:e.duration})]})}))})]})})})},D=n(36),P=function(e){var t=e.IndInfoData;return Object(h.jsx)("section",{id:"page-content",className:"spacer p-bottom-lg",children:Object(h.jsx)("div",{id:"blog",children:Object(h.jsxs)("div",{className:"industry wrapper",children:[Object(h.jsx)("h4",{children:"Experience"}),Object(h.jsx)("div",{className:"eduacation",children:t.map((function(e){return Object(h.jsxs)(h.Fragment,{children:[Object(h.jsx)("h6",{children:e.institution}),Object(h.jsxs)("p",{children:[e.position," ",""!==e.advisor?Object(h.jsx)("span",{children:e.advisor}):null]}),Object(h.jsxs)("p",{className:"date",children:[e.location," | ",e.duration]})]})}))})]})})})},E=n(37),J=function(){return document.body.classList.add("home"),document.body.classList.add("bg-fixed"),document.body.classList.add("bg-line"),Object(h.jsxs)(i.Fragment,{children:[Object(h.jsxs)(d.a,{children:[Object(h.jsx)("meta",{charSet:"UTF-8"}),Object(h.jsx)("title",{children:"SeungHeon Doh | MIR, ML/DL Researcher"}),Object(h.jsx)("link",{rel:"icon",href:"data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>\ud83c\udfb9</text></svg>"}),Object(h.jsx)("meta",{httpEquiv:"x-ua-compatible",content:"ie=edge"}),Object(h.jsx)("meta",{name:"viewport",content:"width=device-width, initial-scale=1"}),Object(h.jsx)("meta",{name:"description",content:"audio, mir, musical informational retrieval"}),Object(h.jsx)("meta",{name:"keywords",content:"audio, mir, musical informational retrieval"}),Object(h.jsx)("meta",{name:"robots",content:"index, follow, noodp"}),Object(h.jsx)("meta",{name:"googlebot",content:"index, follow"}),Object(h.jsx)("meta",{name:"google",content:"notranslate"}),Object(h.jsx)("meta",{name:"format-detection",content:"telephone=no"})]}),Object(h.jsx)(u,{}),Object(h.jsxs)("main",{id:"main",className:"site-main",children:[Object(h.jsx)(j,{ProfData:p}),Object(h.jsx)(x,{NewsInfoData:f}),Object(h.jsx)(T,{}),Object(h.jsx)(P,{IndInfoData:E}),Object(h.jsx)(L,{EduInfoData:D}),Object(h.jsx)(A,{ExpInfoData:I})]}),Object(h.jsx)(m,{})]})},R=n(38),_=n(23),W=function(e){var t=e.item,n=e.key;return Object(h.jsx)("section",{title:t.title,className:"blogTable-item active "+t.category,children:Object(h.jsxs)("div",{className:"contents_div",children:[Object(h.jsx)("a",{href:t.url,cursor:"pointer",children:Object(h.jsx)("img",{className:"img_div",src:"/assets/blog/"+t._id+"/thumbnail.png",alt:t._id})}),Object(h.jsxs)("div",{className:"info_div",children:[Object(h.jsxs)("div",{children:[Object(h.jsx)("h5",{children:Object(h.jsx)("a",{href:t.url,cursor:"pointer",children:t.title})}),Object(h.jsx)("p",{className:"desc",children:t.decs})]}),Object(h.jsx)("div",{className:"date_div",children:Object(h.jsx)("p",{className:"blogdate",children:t.year})})]})]})},n)},F=function(e){Object(y.a)(n,e);var t=Object(w.a)(n);function n(e){var i;return Object(O.a)(this,n),(i=t.call(this,e)).onFilterChange=function(e){var t=i.grid;void 0===i.iso&&(i.iso=new M.a(t,{itemSelector:".blogTable-item",masonry:{horizontalOrder:!0}})),"*"===e?i.iso.arrange({filter:"*"}):i.iso.arrange({filter:".".concat(e)})},i.onFilterChange=i.onFilterChange.bind(Object(k.a)(i)),i.state={selected:0,list:R},i}return Object(v.a)(n,[{key:"handleClick",value:function(e,t){return t.preventDefault(),this.setState({selected:e}),!1}},{key:"componentDidMount",value:function(){}},{key:"render",value:function(){var e=this,t=this.state.list.length-1;return Object(h.jsx)("div",{className:"blogTable spacer p-bottom-lg",children:Object(h.jsxs)("div",{className:"blogTable-wrapper",children:[Object(h.jsx)("h3",{children:"Blogs"}),Object(h.jsx)("ul",{className:"blogTable-filter",children:this.state.list.map((function(n,i){return Object(h.jsxs)(o.a.Fragment,{children:[Object(h.jsx)("li",{children:Object(h.jsx)("span",{title:n.title,className:"btn btn-link transform-scale-h click"+(i===e.state.selected?" active":""),"data-filter":n.filter,onClick:function(t){e.onFilterChange(n.filter),e.handleClick(i,t)},children:n.title})}),i!==t?Object(h.jsx)("li",{children:Object(h.jsx)("span",{className:"btn btn-link",children:"-"})}):""]},i)}))}),Object(h.jsx)("div",{className:"blogTable-item-wrapper",children:Object(h.jsx)("div",{className:"blogTable-items",ref:function(t){return e.grid=t},children:_&&_.map((function(e,t){return e.material,Object(h.jsx)(W,{item:e},t)}))})})]})})}}]),n}(i.Component),H=function(){return document.body.classList.add("blog"),document.body.classList.add("bg-fixed"),document.body.classList.add("bg-line"),Object(h.jsxs)(i.Fragment,{children:[Object(h.jsxs)(d.a,{children:[Object(h.jsx)("meta",{charSet:"UTF-8"}),Object(h.jsx)("title",{children:"SeungHeon Doh | MIR Researcher, ML/DL Engineer"}),Object(h.jsx)("link",{rel:"icon",href:"data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>\ud83c\udfb9</text></svg>"}),Object(h.jsx)("meta",{httpEquiv:"x-ua-compatible",content:"ie=edge"}),Object(h.jsx)("meta",{name:"viewport",content:"width=device-width, initial-scale=1"}),Object(h.jsx)("meta",{name:"description",content:""}),Object(h.jsx)("meta",{name:"keywords",content:""}),Object(h.jsx)("meta",{name:"robots",content:"index, follow, noodp"}),Object(h.jsx)("meta",{name:"googlebot",content:"index, follow"}),Object(h.jsx)("meta",{name:"google",content:"notranslate"}),Object(h.jsx)("meta",{name:"format-detection",content:"telephone=no"})]}),Object(h.jsx)(u,{}),Object(h.jsx)("main",{id:"main",className:"site-main",children:Object(h.jsx)("div",{className:"wrapper",children:Object(h.jsx)(F,{})})}),Object(h.jsx)(m,{})]})},K=function(){return document.body.classList.add("thesis"),document.body.classList.add("bg-fixed"),document.body.classList.add("bg-line"),Object(h.jsxs)(i.Fragment,{children:[Object(h.jsxs)(d.a,{children:[Object(h.jsx)("meta",{charSet:"UTF-8"}),Object(h.jsx)("title",{children:"SeungHeon Doh | MIR, ML/DL Researcher"}),Object(h.jsx)("link",{rel:"icon",href:"data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>\ud83c\udfb9</text></svg>"}),Object(h.jsx)("meta",{httpEquiv:"x-ua-compatible",content:"ie=edge"}),Object(h.jsx)("meta",{name:"viewport",content:"width=device-width, initial-scale=1"}),Object(h.jsx)("meta",{name:"description",content:""}),Object(h.jsx)("meta",{name:"keywords",content:""}),Object(h.jsx)("meta",{name:"robots",content:"index, follow, noodp"}),Object(h.jsx)("meta",{name:"googlebot",content:"index, follow"}),Object(h.jsx)("meta",{name:"google",content:"notranslate"}),Object(h.jsx)("meta",{name:"format-detection",content:"telephone=no"})]}),Object(h.jsx)(u,{}),Object(h.jsx)("main",{id:"main",className:"site-main",children:Object(h.jsx)("div",{className:"wrapper",children:Object(h.jsx)("p",{children:"Comming Soon"})})}),Object(h.jsx)(m,{})]})};var G=function(){return Object(h.jsxs)(c.a,{basename:"",children:[Object(h.jsx)(r.b,{exact:!0,path:"/",component:J}),Object(h.jsx)(r.b,{path:"/blog",component:H}),Object(h.jsx)(r.b,{path:"/thesis",component:K}),Object(h.jsx)(r.a,{to:"/"})]})};n(62),Boolean("localhost"===window.location.hostname||"[::1]"===window.location.hostname||window.location.hostname.match(/^127(?:\.(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)){3}$/));s.a.render(Object(h.jsx)(G,{}),document.getElementById("root")),"serviceWorker"in navigator&&navigator.serviceWorker.ready.then((function(e){e.unregister()})).catch((function(e){console.error(e.message)}))}},[[63,1,2]]]);
//# sourceMappingURL=main.3488e014.chunk.js.map